# Requirements

## To do

Implement and train a better emotion recognition model
Find a better face tracking module to track faces that are not facing the camera

## What are we making

This project is a visual emotion tracker. Our goal is to be able to track specific actions by the users like keeping eye contact for example. The application is easy to use and provides detailed information on the actions of the users. The user can interact with the data collected by the web application to know what actions they are committing the most during conversation. The data provided should be displayed intuitively without being overwhelming for the user to digest. Overall, the idea of the application can be summed up in one sentence: Giving users a platform that can give them an assessment of their interactions in certain dialogues like that of an interview.

## What does an MVP look like

The user should be able to go onto the site and see a video feed of themselves. The application will begin to track the users actions once the feed starts. The user will see a feed of their emotions, in real time, displayed on a Pentagon Graph. As the emotions change, so do the positions of the points on the Pentagon Graph. Emotions displayed are TBD. The user would also see statistics of their top conditions from their session. For example, the user was "unsure" for x amount of time during the 20-minute session they just completed, which accounts for x percentage of the session.

## Technical issues needed to be discussed

* Are the users allowed to save information/data from previous sessions for reference on their progress? Where is this data saved and how do the users access it?
* What will the tech stack consist of?
* Where is the site being hosted?